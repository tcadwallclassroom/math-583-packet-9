---
title: "Packet 9 - Numerical Inference Lightning Round"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
library(infer)
library(gssr)
knitr::opts_chunk$set(echo = T, 
                      cache = T, 
                      eval = T, 
                      cache.lazy = F, 
                      warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```

## Test for Inference on Numeric Variables

We'll start by loading up some data from the GSS:

```{r gss18}
num_vars <- c("age", "coninc", "hrs1")
cat_vars <- c("sex", "satfin", "class", "wrkstat")
my_vars <- c(num_vars, cat_vars)

gss18 <- gss_get_yr(2018)
data <- gss18
data <- data %>% 
  select(all_of(my_vars)) %>% 
  mutate(
    across(everything(), haven::zap_missing),
    across(all_of(cat_vars), forcats::as_factor)
  ) %>% 
  mutate(
    across(all_of(num_vars), as.numeric)
  ) ## GGS numeric variables don't play nicely with infer.
```

The `hrs1` variable records how many hours the respondent worked in the previous week. 

Let's examine the data before doing anything else:

```{r}
data %>% 
  drop_na(hrs1) %>% 
  summarize(mean = mean(hrs1),
            sd = sd(hrs1),
            median = median(hrs1)
            )

data %>% ggplot(aes(x = hrs1)) + 
  geom_histogram(color = "black",
                 fill = "forestgreen",
                 bins = 20)
```


Let's find the mean for our sample:

```{r hrs1 mean}
x_bar <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "mean")
x_bar
```

We can bootstrap a confidence interval for a range estimate:

```{r hrs1 ci}
boot_dist <- data %>%
  specify(response = hrs1) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist)
percentile_ci

boot_dist %>% visualize() +
  shade_confidence_interval(endpoints = percentile_ci)
```

Let's test the claim that the average American works more than 40 hours per week. That is, we are testing the null hypothesis that $\mu = 40$ and the alternative that $\mu > 40$. We'll do this by again bootstrapping our data, to estimate the variance.

```{r hrs1 rando test}
null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

null_dist %>% visualize() +
  shade_p_value(obs_stat = x_bar, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_bar, direction = "greater")
```

The "theoretical distribution" for a sample mean is the *t-statistic*. The idea is that we can't directly compute the z-score, since we don't know the standard deviation $\sigma$. Instead, we can only approximate the standard error with $s$, the sample standard deviation:

\[SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}} \]

The t-score is calculated the same as a z-score:

\[t = \frac{\bar{x}-\mu}{SE}\]

Because the SE is calculated with $s$ rather than $\sigma$, there is a bit more variability in the t-distribution compared to the normal (z-) distribution. In other words, the t-distribution has a slightly lower peak and slightly fatter tails than the normal distribution. (See Introduction to Modern Statistics, p. 362.) The more data we have, the more degrees of freedom in the t-distribution, and the more the t-distribution resembles a normal curve.

We can have `infer` calculate a t-statistic, and compute a null distribution based on t as well:

```{r}
t <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "t", mu = 40)
t

null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "t")

null_dist %>% visualize(method = "both") +
  shade_p_value(obs_stat = t, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = t, direction = "greater")
```

The theoretical t-test can also be run with an R function:

```{r}
ttest1 <- t.test(data$hrs1, mu = 40, alternative = "greater")
ttest1
ttest2 <- data %>% 
  t_test(hrs1 ~ NULL, alternative = "greater")
ttest2
```

As an aside, we can also test hypotheses involving the median. This is especially useful with highly skewed data. We don't have a "t-distribution" for the median, but we can still perform a randomization test:

```{r}
x_tilde <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "median")
x_tilde

null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", med = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "median")

null_dist %>% visualize() +
  shade_p_value(obs_stat = x_tilde, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_tilde, direction = "greater")
```

What do you notice/wonder about our results?

## Wildly non-normal data

Let's do the same thing with the `poker` dataset from the OpenIntro package:

```{r poker, eval=FALSE}
library(openintro)
help("poker")
glimpse(poker)
```

The `winnings` variable indicates the amount of money won by a professional poker player on 50 randomply chosen days.

Let's examine the data before doing anything else:

```{r}
poker %>% 
  drop_na(winnings) %>% 
  summarize(mean = mean(winnings),
            sd = sd(winnings),
            median = median(winnings)
            )

poker %>% ggplot(aes(x = winnings)) + 
  geom_histogram(color = "black",
                 fill = "forestgreen",
                 bins = 30) +
  labs(
    x = "Winnings",
    y = "Number of Days in Sample"
  )

```


Let's find the mean for our sample:

```{r poker mean}
x_bar <- poker %>% 
  specify(response = winnings) %>%
  calculate(stat = "mean")
x_bar
```

We can bootstrap a confidence interval for a range estimate:

```{r poker ci}
boot_dist <- poker %>%
  specify(response = winnings) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist)
percentile_ci

boot_dist %>% visualize() +
  shade_confidence_interval(endpoints = percentile_ci)
```

Let's test the claim that the average winnings are over \$0 (that is, the professional poker player is actually making money!). That is, we are testing the null hypothesis that $\mu = 0$ and the alternative that $\mu > 0$. We'll do this by again bootstrapping our data, to estimate the variance.

```{r poker rando test}
null_dist <- poker %>%
  specify(response = winnings) %>%
  hypothesize(null = "point", mu = 0) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

null_dist %>% visualize() +
  shade_p_value(obs_stat = x_bar, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_bar, direction = "greater")
```

Let's see what we get from the t-statistic, and compute a null distribution based on t as well:

```{r}
t <- poker %>% 
  specify(response = winnings) %>%
  calculate(stat = "t", mu = 0)
t

null_dist <- poker %>% 
  specify(response = winnings) %>%
  hypothesize(null = "point", mu = 0) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "t")

null_dist %>% visualize(method = "both") +
  shade_p_value(obs_stat = t, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = t, direction = "greater")
```

What do you notice/wonder?

## Difference in means

Here's another possible question: are unemployed/laid off workers younger, on average, than full-time workers? We can calculate a *difference in means* for these groups, and determine if the difference is significant:

```{r}
data %>% 
  drop_na(age) %>% 
  group_by(wrkstat) %>% 
  summarise(mean_age = mean(age))

data %>% filter(wrkstat == "working fulltime") %>% 
  drop_na(age) %>% 
  summarize(mean_age = mean(age))

data %>% filter(wrkstat == "unempl, laid off") %>% 
  drop_na(age) %>% 
  summarize(mean_age = mean(age))

data %>% filter(wrkstat == "working fulltime" |
                  wrkstat == "unempl, laid off") %>% 
  drop_na(age) %>% 
  ggplot(aes(x = age)) +
  geom_histogram(aes(fill = wrkstat),
                 color = "black",
                 bins = 20) +
  labs(
    x = "Age",
    y = "Number of respondents"
  )
```

