---
title: "Packet 9 - Numerical Inference Lightning Round"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
library(infer)
knitr::opts_chunk$set(echo = T, 
                      cache = T, 
                      eval = T, 
                      cache.lazy = F, 
                      warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```

## Test for Inference on Numeric Variables

We'll start by loading up some data from the GSS:

```{r gss18}
num_vars <- c("coninc", "hrs1")
cat_vars <- c("sex", "satfin", "class", "wrkstat")
my_vars <- c(num_vars, cat_vars)

gss18 <- gss_get_yr(2018)
data <- gss18
data <- data %>% 
  select(all_of(my_vars)) %>% 
  mutate(
    across(everything(), haven::zap_missing),
    across(all_of(cat_vars), forcats::as_factor)
  ) %>% 
  mutate(
    coninc = as.numeric(coninc),
    hrs1 = as.numeric(hrs1)
  ) ## The GSS data sometimes has trouble with numeric variables.
```

The `hrs1` variable records how many hours the respondent worked in the previous week. Let's find the mean for our sample:

```{r hrs1 mean}
data %>% drop_na(hrs1) %>% summarize(mean(hrs1))

x_bar <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "mean")
x_bar
```

We can bootstrap a confidence interval for a range estimate:

```{r hrs1 ci}
boot_dist <- data %>%
  specify(response = hrs1) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist)
percentile_ci

visualize(boot_dist) +
  shade_confidence_interval(endpoints = percentile_ci)
```

Let's test the claim that the average American works more than 40 hours per week. That is, we are testing the null hypothesis that $\mu = 40$ and the alternative that $\mu > 40$. We'll do this by again bootstrapping our data, to estimate the variance.

```{r hrs1 rando test}
null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

visualize(null_dist) +
  shade_p_value(obs_stat = x_bar, direction = "two-sided")

null_dist %>%
  get_p_value(obs_stat = x_bar, direction = "two-sided")
```

The "theoretical distribution" for a sample mean is the *t-statistic*. The idea is that we can't directly compute the z-score, since we don't know the standard deviation $\sigma$. Instead, we can only approximate the standard error with $s$, the sample standard deviation:

\[SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}} \]

The t-score is calculated the same as a z-score:

\[t = \frac{\bar{x}-\mu}{SE}\]

Because the SE is calculated with $s$ rather than $\sigma$, there is a bit more variability in the t-distribution compared to the normal (z-) distribution. In other words, the t-distribution has a slightly lower peak and slightly fatter tails than the normal distribution. (See Introduction to Modern Statistics, p. 362)

