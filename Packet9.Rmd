---
title: "Packet 9 - Numerical Inference Lightning Round"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
library(infer)
library(gssr)
knitr::opts_chunk$set(echo = T, 
                      cache = T, 
                      eval = T, 
                      cache.lazy = F, 
                      warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```

## Test for Inference on Numeric Variables

We'll start by loading up some data from the GSS:

```{r gss18}
num_vars <- c("age", "coninc", "hrs1")
cat_vars <- c("sex", "satfin", "class", "wrkstat")
my_vars <- c(num_vars, cat_vars)

gss18 <- gss_get_yr(2018)
data <- gss18
data <- data %>% 
  select(all_of(my_vars)) %>% 
  mutate(
    across(everything(), haven::zap_missing),
    across(all_of(cat_vars), forcats::as_factor)
  ) %>% 
  mutate(
    across(all_of(num_vars), as.numeric)
  ) ## GGS numeric variables don't play nicely with infer.
```

The `hrs1` variable records how many hours the respondent worked in the previous week. 

Let's examine the data before doing anything else:

```{r}
data %>% 
  drop_na(hrs1) %>% 
  summarize(mean = mean(hrs1),
            sd = sd(hrs1),
            median = median(hrs1)
            )

data %>% ggplot(aes(x = hrs1)) + 
  geom_histogram(color = "black",
                 fill = "forestgreen",
                 bins = 20)
```


Let's find the mean for our sample:

```{r hrs1 mean}
x_bar <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "mean")
x_bar
```

We can bootstrap a confidence interval for a range estimate:

```{r hrs1 ci}
boot_dist <- data %>%
  specify(response = hrs1) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist, level = 0.95)
percentile_ci

boot_dist %>% visualize(bins = 30) +
  shade_confidence_interval(endpoints = percentile_ci)
```

Let's test the claim that the average American works more than 40 hours per week. That is, we are testing the null hypothesis that $\mu = 40$ and the alternative that $\mu > 40$. We'll do this by again bootstrapping our data, to estimate the variance.

```{r hrs1 rando test}
null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

null_dist %>% visualize(bins = 30) +
  shade_p_value(obs_stat = x_bar, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_bar, direction = "greater")
```

The "theoretical distribution" for a sample mean is the *t-statistic*. The idea is that we can't directly compute the z-score, since we don't know the standard deviation $\sigma$. Instead, we can only approximate the standard error with $s$, the sample standard deviation:

\[SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}} \]

The t-score is calculated the same as a z-score:

\[t = \frac{\bar{x}-\mu}{SE}\]

Because the SE is calculated with $s$ rather than $\sigma$, there is a bit more variability in the t-distribution compared to the normal (z-) distribution. In other words, the t-distribution has a slightly lower peak and slightly fatter tails than the normal distribution. (See Introduction to Modern Statistics, p. 362.) The more data we have, the more degrees of freedom in the t-distribution, and the more the t-distribution resembles a normal curve.

We can have `infer` calculate a t-statistic, and compute a null distribution based on t as well:

```{r}
t <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "t", mu = 40)
t

null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "t")

null_dist %>% visualize(method = "both", bins = 30) +
  shade_p_value(obs_stat = t, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = t, direction = "greater")
```

The theoretical t-test can also be run with an R function:

```{r}
ttest1 <- t.test(data$hrs1, mu = 40, alternative = "greater")
ttest1
ttest2 <- data %>% 
  t_test(hrs1 ~ NULL, alternative = "greater")
ttest2
```

As an aside, we can also test hypotheses involving the median. This is especially useful with highly skewed data. We don't have a "t-distribution" for the median, but we can still perform a randomization test:

```{r}
x_tilde <- data %>% 
  specify(response = hrs1) %>%
  calculate(stat = "median")
x_tilde

null_dist <- data %>%
  specify(response = hrs1) %>%
  hypothesize(null = "point", med = 40) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "median")

null_dist %>% visualize(bins = 30) +
  shade_p_value(obs_stat = x_tilde, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_tilde, direction = "greater")
```

What do you notice/wonder about our results?

## Wildly non-normal data

Let's do the same thing with the `poker` dataset from the OpenIntro package:

```{r poker, eval=FALSE}
library(openintro)
help("poker")
glimpse(poker)
```

The `winnings` variable indicates the amount of money won by a professional poker player on 50 randomply chosen days.

Let's examine the data before doing anything else:

```{r}
poker %>% 
  drop_na(winnings) %>% 
  summarize(mean = mean(winnings),
            sd = sd(winnings),
            median = median(winnings)
            )

poker %>% ggplot(aes(x = winnings)) + 
  geom_histogram(color = "black",
                 fill = "forestgreen",
                 bins = 30) +
  labs(
    x = "Winnings",
    y = "Number of Days in Sample"
  )

```


Let's find the mean for our sample:

```{r poker mean}
x_bar <- poker %>% 
  specify(response = winnings) %>%
  calculate(stat = "mean")
x_bar
```

We can bootstrap a confidence interval for a range estimate:

```{r poker ci}

boot_dist <- poker %>%
  specify(response = winnings) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist)
percentile_ci

boot_dist %>% visualize(bins = 30) +
  shade_confidence_interval(endpoints = percentile_ci)
```

Let's test the claim that the average winnings are over \$0 (that is, the professional poker player is actually making money!). That is, we are testing the null hypothesis that $\mu = 0$ and the alternative that $\mu > 0$. We'll do this by again bootstrapping our data, to estimate the variance.

```{r poker rando test}
null_dist <- poker %>%
  specify(response = winnings) %>%
  hypothesize(null = "point", mu = 0) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

null_dist %>% visualize(bins = 30) +
  shade_p_value(obs_stat = x_bar, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = x_bar, direction = "greater")
```

Let's see what we get from the t-statistic, and compute a null distribution based on t as well:

```{r}
t <- poker %>% 
  specify(response = winnings) %>%
  calculate(stat = "t", mu = 0)
t

null_dist <- poker %>% 
  specify(response = winnings) %>%
  hypothesize(null = "point", mu = 0) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "t")

null_dist %>% visualize(method = "both",bins = 30) +
  shade_p_value(obs_stat = t, direction = "greater")

null_dist %>%
  get_p_value(obs_stat = t, direction = "greater")
```

What do you notice/wonder?

## T-test for paired data

Take a look at the `textbooks` data set in `openintro`. We are interested in the question, "Are textbooks more expensive at the UCLA bookstore than they are at Amazon?"

```{r}
x_bar <- textbooks %>% 
  specify(response = diff) %>% 
  calculate(stat = "mean")
x_bar

null_dist <- textbooks %>% 
  specify(response = diff) %>%
  hypothesize(null = "point", mu = 0) %>% 
  generate(reps = 10000, type = "bootstrap") %>% 
  calculate(stat = "mean")

null_dist %>% visualise(bins = 30)+
  shade_p_value(obs_stat = x_bar, direction = "two-sided")

null_dist %>% get_p_value(obs_stat = x_bar, direction = "two-sided")
```


## Difference in means

Here's another possible question: are unemployed/laid off workers younger, on average, than full-time workers? We can calculate a *difference in means* for these groups, and determine if the difference is significant:

```{r}
data <- data %>% 
  drop_na(age)

data %>% 
  group_by(wrkstat) %>% 
  summarise(mean_age = mean(age))

data %>% filter(wrkstat == "working fulltime") %>% 
  summarize(mean_age = mean(age))

data %>% filter(wrkstat == "unempl, laid off") %>% 
  summarize(mean_age = mean(age))

data %>% filter(wrkstat == "working fulltime" |
                  wrkstat == "unempl, laid off") %>% 
  ggplot(aes(x = age)) +
  geom_histogram(aes(fill = wrkstat),
                 color = "black",
                 binwidth = 2) +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    x = "Age",
    y = "Number of respondents",
    fill = "Work Status"
  )
```

Using `infer`:

```{r}

data_twogroups <- data %>% 
  filter(wrkstat == "working fulltime" |
         wrkstat == "unempl, laid off") %>% 
  droplevels()

table(data_twogroups$wrkstat)

d_hat <- data_twogroups %>% 
  specify(response = age, explanatory = wrkstat) %>%
  calculate(stat = "diff in means")
d_hat

### Bootstrap Conf. Interval

boot_dist_d_hat <- data_twogroups %>%
  specify(response = age, explanatory = wrkstat) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "diff in means")

percentile_ci <- get_ci(boot_dist_d_hat, level = 0.95)
percentile_ci

boot_dist_d_hat %>% visualize(bins = 30) +
  shade_confidence_interval(endpoints = percentile_ci)


### Hypothesis test: Null: d_hat = 0

null_dist_d_hat <- data_twogroups %>%
  specify(response = age, explanatory = wrkstat) %>%
  hypothesise(null = "independence") %>% 
  generate(reps = 10000, type = "permute") %>%
  calculate(stat = "diff in means")

null_dist_d_hat %>% visualise(bins = 30) +
  shade_p_value(obs_stat = d_hat, direction = "two-sided")

null_dist_d_hat %>% 
  get_p_value(obs_stat = d_hat, direction = "two-sided")

### T-test:

t <- data_twogroups %>% 
  specify(response = age, explanatory = wrkstat) %>%
  calculate(stat = "t")
t

null_dist_t <- data_twogroups %>%
  specify(response = age, explanatory = wrkstat) %>%
  hypothesise(null = "independence") %>% 
  generate(reps = 10000, type = "permute") %>%
  calculate(stat = "t")

null_dist_t %>% visualise(bins = 30, method = "both") +
  shade_p_value(obs_stat = t, direction = "two-sided")

null_dist_t %>% 
  get_p_value(obs_stat = t, direction = "two-sided")
```



## Analysis of Variance (ANOVA)

Look again at the GSS data on work status and age. Rather than examine each pair of groups, we want to test the null hypothesis of whther these two variables are independent across all the groups. In order ot examine this, we'll conduct an Analysis of Variance, also known as an ANOVA or F-test.

The test statistic, F, is defined as the *ratio of the mean square between groups (MSG) and the mean square error (MSE)*. :

\begin{align*}
F &= \frac{MSG}{MSE}
\end{align}

Both MSE and MSG are complicated to calculate, but roughly speaking, MSG is the variance *between* group means, where MSE is the variance *within*  groups. 

```{r}
F_stat <- data %>% 
  specify(response = age, explanatory = wrkstat) %>% 
  calculate(stat = "F")

F_null_dist<- data %>% 
  specify(response = age, explanatory = wrkstat) %>% 
  hypothesise(null = "independence") %>% 
  generate(reps = 10000, type = "permute") %>% 
  calculate(stat = "F")

F_null_dist %>% visualise(bins = 30, method = "both") +
  shade_p_value(obs_stat = F_stat, direction = "greater")

F_null_dist %>% 
  get_p_value(obs_stat = F_stat, direction = "greater")
```

You can run an ANOVA in R using the code:

```{r}
data %>% 
    group_by(wrkstat) %>% 
    summarise(mean_age = mean(age))

model <- lm(age ~ wrkstat, data = data)
model

anova(model)
```
